{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1653545d-5a02-4c43-b0b4-857579f38c8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "762b50f5-2b76-4135-8388-d5c46b02567e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# St. Louis area universities\n",
    "universities = [\n",
    "    {\n",
    "        'name': 'Washington University in St. Louis',\n",
    "        'url': 'https://en.wikipedia.org/wiki/Washington_University_in_St._Louis'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Saint Louis University',\n",
    "        'url': 'https://en.wikipedia.org/wiki/Saint_Louis_University'\n",
    "    },\n",
    "    {\n",
    "        'name': 'University of Missouri–St. Louis',\n",
    "        'url': 'https://en.wikipedia.org/wiki/University_of_Missouri%E2%80%93St._Louis'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Webster University',\n",
    "        'url': 'https://en.wikipedia.org/wiki/Webster_University'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Maryville University',\n",
    "        'url': 'https://en.wikipedia.org/wiki/Maryville_University'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Missouri Baptist University',\n",
    "        'url': 'https://en.wikipedia.org/wiki/Missouri_Baptist_University'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Harris–Stowe State University',\n",
    "        'url': 'https://en.wikipedia.org/wiki/Harris%E2%80%93Stowe_State_University'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Lindenwood University',\n",
    "        'url': 'https://en.wikipedia.org/wiki/Lindenwood_University'\n",
    "    }\n",
    "]\n",
    "\n",
    "# User agent will help with scraping by making the request look more like a real browser request rather than a bot or script. \n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "}\n",
    "\n",
    "# Creating a data list as a bucket for the scraped data. \n",
    "all_data = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71d0d2eb-b88e-46bc-a0d8-18d901720fa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for university in universities:\n",
    "    try:\n",
    "        print(f\"Scraping: {university['name']}...\")\n",
    "        \n",
    "        response = requests.get(university['url'], headers=headers, timeout=30)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Find the infobox\n",
    "            infobox = soup.find('table', class_='infobox')\n",
    "            \n",
    "            enrollment = None\n",
    "            city = None\n",
    "            state = None\n",
    "            coordinates = None\n",
    "            \n",
    "            if infobox:\n",
    "                rows = infobox.find_all('tr')\n",
    "                \n",
    "                for row in rows:\n",
    "                    header = row.find('th')\n",
    "                    \n",
    "                    if header:\n",
    "                        header_text = header.text.strip().lower()\n",
    "                        value = row.find('td')\n",
    "                        \n",
    "                        # Extract enrollment\n",
    "                        if 'student' in header_text and value:\n",
    "                            text = value.text.strip()\n",
    "                            match = re.search(r'([\\d,]+)', text)\n",
    "                            if match:\n",
    "                                enrollment = match.group(1)\n",
    "                        \n",
    "                        # Extract location and coordinates\n",
    "                        if 'location' in header_text and value:\n",
    "                            # Get full text\n",
    "                            location_text = value.get_text(separator=' ', strip=True)\n",
    "                            \n",
    "                            # Extract coordinates BEFORE cleaning\n",
    "                            coord_match = re.search(r'(\\d+\\.?\\d*°\\s*[NS]\\s+\\d+\\.?\\d*°\\s*[EW])', location_text)\n",
    "                            if coord_match:\n",
    "                                coordinates = coord_match.group(1)\n",
    "                            \n",
    "                            # Clean location text\n",
    "                            # Remove citations [1], [2], etc.\n",
    "                            location_clean = re.sub(r'\\[.*?\\]', '', location_text)\n",
    "                            \n",
    "                            # Remove coordinates\n",
    "                            location_clean = re.sub(r'\\d+\\.?\\d*°.*', '', location_clean)\n",
    "                            \n",
    "                            # Remove \"postal address\" phrase\n",
    "                            location_clean = re.sub(r'postal address', '', location_clean, flags=re.IGNORECASE)\n",
    "                            \n",
    "                            # Remove \"United States\" and variations\n",
    "                            location_clean = re.sub(r',?\\s*U\\.?S\\.?A?\\.?', '', location_clean, flags=re.IGNORECASE)\n",
    "                            location_clean = re.sub(r',?\\s*United States', '', location_clean, flags=re.IGNORECASE)\n",
    "                            \n",
    "                            # Remove street addresses (patterns like numbers followed by street names)\n",
    "                            location_clean = re.sub(r'\\d+\\s+[\\w\\s]+(?:Drive|Street|Road|Avenue|Lane|Boulevard|Way|Court|Dr|St|Rd|Ave|Ln|Blvd)\\s*,?\\s*', '', location_clean, flags=re.IGNORECASE)\n",
    "                            \n",
    "                            # Remove zip codes (5 digits or 5+4 format)\n",
    "                            location_clean = re.sub(r'\\b\\d{5}(?:-\\d{4})?\\b', '', location_clean)\n",
    "                            \n",
    "                            # Clean up extra spaces and commas\n",
    "                            location_clean = re.sub(r'\\s+', ' ', location_clean)  # Multiple spaces to single\n",
    "                            location_clean = re.sub(r',\\s*,', ',', location_clean)  # Double commas\n",
    "                            location_clean = location_clean.strip(' ,')  # Leading/trailing spaces and commas\n",
    "                            \n",
    "                            # Split into city and state\n",
    "                            # Expected format: \"City, State\" or just \"City\"\n",
    "                            if ',' in location_clean:\n",
    "                                parts = location_clean.split(',')\n",
    "                                city = parts[0].strip()\n",
    "                                state = parts[1].strip() if len(parts) > 1 else None\n",
    "                            else:\n",
    "                                city = location_clean.strip()\n",
    "                                state = None\n",
    "\n",
    "                            # Clean up state so the column all has the same value. \n",
    "                            if state == 'MO': \n",
    "                                state = 'Missouri'\n",
    "            \n",
    "            all_data.append({\n",
    "                'University': university['name'],\n",
    "                'City': city,\n",
    "                'State': state,\n",
    "                'Coordinates': coordinates,\n",
    "                'Student Enrollment': enrollment,\n",
    "                'Source URL': university['url']\n",
    "            })\n",
    "            \n",
    "            print(f\"  ✓ City: {city}\")\n",
    "            print(f\"  ✓ State: {state}\")\n",
    "            print(f\"  ✓ Coordinates: {coordinates}\")\n",
    "            print(f\"  ✓ Enrollment: {enrollment}\\n\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"  ✗ Failed with status {response.status_code}\\n\")\n",
    "            all_data.append({\n",
    "                'University': university['name'],\n",
    "                'City': None,\n",
    "                'State': None,\n",
    "                'Coordinates': None,\n",
    "                'Student Enrollment': None,\n",
    "                'Source URL': university['url']\n",
    "            })\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {e}\\n\")\n",
    "        all_data.append({\n",
    "            'University': university['name'],\n",
    "            'City': None,\n",
    "            'State': None,\n",
    "            'Coordinates': None,\n",
    "            'Student Enrollment': None,\n",
    "            'Source URL': university['url']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96883486-ed7d-4382-80c0-f4a623932a94",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"Source URL\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}}}},\"syncTimestamp\":1770951390384}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create DataFrame with all of the scraped data\n",
    "df_scraped = pd.DataFrame(all_data)\n",
    "df_scraped.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f862b0d0-a0d7-476c-a489-54ffefaac6f9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fix SyntaxError in base_url assignment"
    }
   },
   "outputs": [],
   "source": [
    "# Connecting to the API using the request module we pulled in earlier. \n",
    "lat = 38.63\n",
    "log = -90.20\n",
    "start_date = \"2026-01-01\"\n",
    "end_date = \"2026-01-31\"\n",
    "base_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "params = {\n",
    "    \"latitude\": lat,\n",
    "    \"longitude\": log,\n",
    "    \"start_date\": start_date,\n",
    "    \"end_date\": end_date,\n",
    "    \"daily\": \"temperature_2m_mean,temperature_2m_max,temperature_2m_min,snowfall_sum,precipitation_sum\",\n",
    "    \"temperature_unit\": \"fahrenheit\",\n",
    "    \"timezone\": \"auto\",\n",
    "    \"precipitation_unit\": \"inch\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "def6dfa0-ac7b-413b-bb6a-48779fec4776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# request the data from the api and save as a data frame as well. \n",
    "weather_response = requests.get(base_url, params=params)\n",
    "weather_data = weather_response.json()\n",
    "weather_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64d8738f-c3dd-4580-88cf-a17fbff21ab0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Transforming my weather data into a data frame\n",
    "df_weather = pd.DataFrame({\n",
    "    'date': weather_data['daily']['time'],\n",
    "    'temp_max': weather_data['daily']['temperature_2m_max'],\n",
    "    'temp_min': weather_data['daily']['temperature_2m_min'],\n",
    "    'temp_mean': weather_data['daily']['temperature_2m_mean'],\n",
    "    'precipitation': weather_data['daily']['precipitation_sum'],\n",
    "    'snowfall': weather_data['daily']['snowfall_sum']\n",
    "})\n",
    "df_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27ac4521-a438-485f-876c-c68968dd3d7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### WEATHER Classifications: \n",
    "1. Average Daily Temp below 20 Degrees F. \n",
    "2. Minimum Temp below 15 Degrees F. \n",
    "3. Snowfall is more than 2 inches in a given day. \n",
    "4. Any day with more than .15 inch of precipitation while AVG Daily temp below freezing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d7ce87d-64d8-4e05-9460-e8059d04cf84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# defining a function to classify severe and non severe weather conditions. \n",
    "\n",
    "def classify_weather(row):\n",
    "  temp_mean = row['temp_mean']\n",
    "  temp_min = row['temp_min']\n",
    "  precipitation = row['precipitation']\n",
    "  snowfall = row['snowfall']\n",
    "\n",
    "  if (temp_mean <= 20) or (temp_min <= 15) or (snowfall >= 2) or (precipitation >= .15 and temp_mean < 32):\n",
    "    return 'severe'\n",
    "  else: \n",
    "    return 'not_severe'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4e43f8e-0960-4be7-942c-54d8f397631f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Using the defined function and applying to the established weather dataframe. \n",
    "df_weather['weather_severity'] = df_weather.apply(classify_weather, axis=1)\n",
    "df_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89a10675-7b49-487a-83cf-bd6b34305b37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculating the number of severe weather days in St. Louis in January. \n",
    "severe_days = (df_weather['weather_severity'] == 'severe').sum()\n",
    "print(f\"Number of severe days: {severe_days}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63fa0ff0-47c2-48bc-95d2-196b2bb95900",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# We need to clean my student enrollment column since it contains a column and is coming over as a string\n",
    "df_scraped['Student Enrollment'] = df_scraped['Student Enrollment'].str.replace(',', '').astype(int)\n",
    "\n",
    "# Defining a function to complete row by row student / severe day calculation. \n",
    "def students_per_severe_day(row):\n",
    "  enrollment = row['Student Enrollment']\n",
    "  student_severe_days = enrollment * severe_days\n",
    "  return student_severe_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d4d8cc2-31b8-4394-8d48-79d7bc548d6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Using the students_per_severe_day function and applying to the existing scrapped data frame. \n",
    "df_scraped['students_per_severe_day'] = df_scraped.apply(students_per_severe_day, axis=1)\n",
    "df_scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27218ec9-6977-45c7-a593-6249d9805417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating the final dataframe. \n",
    "df_final = pd.DataFrame()\n",
    "df_final['University'] = df_scraped['University']\n",
    "df_final['State'] = df_scraped['State']\n",
    "df_final['Enrolled_Students'] = df_scraped['Student Enrollment']\n",
    "df_final['Severe_Days'] = severe_days\n",
    "df_final['Severe_Days_Per_Student'] = df_scraped['students_per_severe_day']\n",
    "df_final['Total_STL_Severe_Student_Days'] = df_scraped['students_per_severe_day'].sum()\n",
    "df_final"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "exercise04.ipynb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
